"""
Differential scanning engine with LLM-powered impact analysis.

Scans affected skill/plugin targets on both BASE and HEAD refs, then uses the
LLM to semantically correlate the findings and determine the true security
impact of the code change.
"""

import json
import shutil
import subprocess
import tempfile
import traceback
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional

from scanner.ci.changed_files import ChangedFile, get_file_patch
from scanner.ci.target_resolver import AffectedTarget
from scanner.core.plugin_parser import ParsedPlugin
from scanner.core.skill_analyzer import SecurityFinding
from scanner.ai.providers import invoke_with_retry, extract_text_content, RateLimiter


# ── Data classes ─────────────────────────────────────────────────────


@dataclass
class ImpactFinding:
    """A finding annotated with its impact status relative to the PR."""

    status: str  # "new", "worsened", "resolved", "unchanged"
    finding: SecurityFinding
    description: str = ""
    severity: str = ""
    paired_finding: Optional[SecurityFinding] = None

    def __post_init__(self):
        if not self.severity:
            self.severity = self.finding.severity


@dataclass
class TargetImpactResult:
    """Impact analysis result for a single affected target."""

    target: AffectedTarget
    new_findings: list[ImpactFinding] = field(default_factory=list)
    worsened_findings: list[ImpactFinding] = field(default_factory=list)
    resolved_findings: list[ImpactFinding] = field(default_factory=list)
    unchanged_findings: list[ImpactFinding] = field(default_factory=list)
    impact_summary: str = ""
    risk_delta: str = "unchanged"  # "increased", "decreased", "unchanged"
    head_verdict: str = "safe"
    base_verdict: Optional[str] = None


@dataclass
class PRScanSummary:
    """Aggregate summary across all affected targets."""

    total_targets_affected: int = 0
    new_count: int = 0
    worsened_count: int = 0
    resolved_count: int = 0
    unchanged_count: int = 0
    overall_risk_delta: str = "unchanged"
    verdict: str = "pass"  # "pass" or "fail"


@dataclass
class PRScanResult:
    """Complete result of a CI/PR differential scan."""

    affected_targets: list[AffectedTarget] = field(default_factory=list)
    target_results: list[TargetImpactResult] = field(default_factory=list)
    summary: PRScanSummary = field(default_factory=PRScanSummary)


# ── Impact analysis prompts ──────────────────────────────────────────

_IMPACT_SYSTEM_PROMPT = """\
You are a security analyst comparing two scan reports of the same AI agent \
plugin/skill — one from BEFORE a code change (base) and one from AFTER (head). \
The findings in each report were generated by an LLM security scanner, so the \
same underlying issue may be described differently between the two reports.

Your job is to determine the TRUE SECURITY IMPACT of the code change by \
correlating findings across the two reports and examining the actual code diffs.

For each finding in the HEAD report, determine:
- Is it a genuinely NEW vulnerability introduced by this change?
- Is it an EXISTING vulnerability already present in base (possibly described differently)?
- Is it an existing vulnerability that got WORSE due to this change?

For each finding in the BASE report, determine:
- Was it RESOLVED/FIXED by this change?
- Is it still present (matched to a HEAD finding)?

Rules:
- Two findings about the SAME vulnerability may have different wording, severity, \
  or framing. Use semantic understanding to match them.
- Use the code diffs to determine causality: if a new finding references code \
  that was changed in the diff, it's likely new or worsened.
- A finding that exists in BOTH reports about unchanged code is "unchanged".
- Be conservative: only mark something as "new" if there's clear evidence in \
  the diff that the change introduced it.

CRITICAL: Respond ONLY with valid JSON. No markdown fences.\
"""

_IMPACT_USER_PROMPT = """\
Analyze the security impact of this code change.

**Change context:**
Target: {target_name} ({target_type})
Scenario: {change_scenario}
Changed files: {changed_files}

**Code diffs:**
{code_diffs}

**BASE scan findings (before change):**
{base_findings_json}

**HEAD scan findings (after change):**
{head_findings_json}

Correlate the findings and determine the true impact. Return JSON:
{{
  "impact_summary": "1-2 sentence summary of the security impact of this change",
  "risk_delta": "increased|decreased|unchanged",
  "new_vulnerabilities": [
    {{
      "head_finding_index": 0,
      "severity": "critical|high|medium|low",
      "title": "Short title",
      "description": "Why this is new — what code change introduced it",
      "category": "category_name",
      "section": "malicious|code_security",
      "component": "component_name",
      "file_path": "path/to/file",
      "line_number": null,
      "code_snippet": "relevant code",
      "remediation": "How to fix"
    }}
  ],
  "worsened_vulnerabilities": [
    {{
      "base_finding_index": 0,
      "head_finding_index": 0,
      "severity": "critical|high|medium|low",
      "title": "Short title",
      "description": "How the change made this existing issue worse",
      "change_detail": "Specific detail about what worsened"
    }}
  ],
  "resolved_vulnerabilities": [
    {{
      "base_finding_index": 0,
      "title": "Short title",
      "description": "How the change fixed this"
    }}
  ],
  "unchanged_vulnerabilities": [
    {{
      "base_finding_index": 0,
      "head_finding_index": 0,
      "title": "Short title",
      "description": "Same issue, present in both versions"
    }}
  ]
}}\
"""

# ── Worktree helpers ─────────────────────────────────────────────────

_MAX_DIFF_CHARS = 15000


def _create_worktree(ref: str, repo_root: str) -> str:
    """Create a temporary git worktree checked out at *ref*."""
    tmp_dir = tempfile.mkdtemp(prefix="scanner-base-")
    cmd = ["git", "worktree", "add", "--detach", tmp_dir, ref]
    result = subprocess.run(
        cmd, capture_output=True, text=True, cwd=repo_root, timeout=60,
    )
    if result.returncode != 0:
        shutil.rmtree(tmp_dir, ignore_errors=True)
        raise RuntimeError(f"git worktree add failed: {result.stderr[:300]}")
    return tmp_dir


def _cleanup_worktree(worktree_path: str, repo_root: str) -> None:
    """Remove a temporary git worktree."""
    try:
        subprocess.run(
            ["git", "worktree", "remove", "--force", worktree_path],
            capture_output=True, text=True, cwd=repo_root, timeout=30,
        )
    except Exception:
        pass
    shutil.rmtree(worktree_path, ignore_errors=True)


def _collect_patches(
    target: AffectedTarget,
    base_ref: str,
    head_ref: str,
    repo_root: str,
) -> str:
    """Collect unified diffs for all changed files in a target, truncated."""
    patches: list[str] = []
    total_chars = 0

    for cf in target.changed_files:
        if cf.patch:
            patch_text = cf.patch
        else:
            patch_text = get_file_patch(base_ref, head_ref, cf.path, repo_root) or ""

        if patch_text:
            header = f"--- {cf.path} [{cf.status}] ---\n"
            remaining = _MAX_DIFF_CHARS - total_chars
            if remaining <= 0:
                patches.append("... (remaining diffs truncated)")
                break
            snippet = patch_text[:remaining]
            patches.append(header + snippet)
            total_chars += len(header) + len(snippet)

    return "\n\n".join(patches) if patches else "(no diffs available)"


# ── Finding serialization ────────────────────────────────────────────


def _findings_to_json(findings: list[SecurityFinding], limit: int = 30) -> str:
    """Serialize findings to compact JSON for the LLM prompt."""
    items = []
    for i, f in enumerate(findings[:limit]):
        items.append({
            "index": i,
            "severity": f.severity,
            "section": f.section,
            "category": f.category or "",
            "rule_id": f.rule_id,
            "rule_name": f.rule_name,
            "message": f.message[:300],
            "component_type": f.component_type,
            "component_name": f.component_name,
            "component_path": f.component_path,
            "line": f.line,
            "snippet": (f.snippet or "")[:200],
        })
    return json.dumps(items, indent=2)


# ── Core scanner ─────────────────────────────────────────────────────


class DiffScanner:
    """Runs differential scans on affected targets and produces impact analysis."""

    def __init__(
        self,
        scanner,  # PluginScanner instance
        llm,
        *,
        ai_provider: str = "openai",
        ai_model: Optional[str] = None,
        verbose: bool = False,
        quiet: bool = False,
        static: bool = False,
        ai_triage_threshold: float = 0.5,
        workers: int = 4,
        rate_limiter: Optional[RateLimiter] = None,
    ):
        self.scanner = scanner
        self.llm = llm
        self.ai_provider = ai_provider
        self.ai_model = ai_model
        self.verbose = verbose
        self.quiet = quiet
        self.static = static
        self.ai_triage_threshold = ai_triage_threshold
        self.workers = workers
        self.rate_limiter = rate_limiter

    def scan_pr(
        self,
        affected_targets: list[AffectedTarget],
        base_ref: str,
        head_ref: str,
        repo_root: str,
    ) -> PRScanResult:
        """Run the full differential scan pipeline for all affected targets."""
        from scanner.main import _run_single_scan_pipeline

        result = PRScanResult(affected_targets=affected_targets)

        for idx, target in enumerate(affected_targets, 1):
            if not self.quiet:
                print(f"\n{'#' * 60}")
                print(f"# TARGET {idx}/{len(affected_targets)}: "
                      f"{target.name} ({target.target_type}) [{target.change_scenario}]")
                print(f"{'#' * 60}")

            try:
                impact = self._scan_target(
                    target, base_ref, head_ref, repo_root, _run_single_scan_pipeline,
                )
                result.target_results.append(impact)
            except Exception as exc:
                if self.verbose:
                    print(f"  [ERROR] Failed to scan {target.name}: {exc}")
                    traceback.print_exc()
                result.target_results.append(TargetImpactResult(
                    target=target,
                    impact_summary=f"Scan failed: {exc}",
                    risk_delta="unchanged",
                ))

        result.summary = self._compute_summary(result)
        return result

    def _scan_target(
        self,
        target: AffectedTarget,
        base_ref: str,
        head_ref: str,
        repo_root: str,
        run_pipeline,
    ) -> TargetImpactResult:
        """Scan a single target on both refs and run impact analysis."""
        head_path = str(Path(repo_root) / target.path)

        # ── New target: HEAD only ──
        if target.change_scenario == "new_target":
            return self._scan_new_target(target, head_path)

        # ── Deleted target: report removal ──
        if target.change_scenario == "deleted_target":
            return self._scan_deleted_target(target, base_ref, repo_root)

        # ── Modified: scan both and run impact analysis ──
        head_findings, head_verdict = self._run_scan(head_path, run_pipeline)

        base_worktree = None
        base_findings: list[SecurityFinding] = []
        base_verdict: Optional[str] = None
        try:
            base_worktree = _create_worktree(base_ref, repo_root)
            base_path = str(Path(base_worktree) / target.path)

            if Path(base_path).exists():
                base_findings, base_verdict = self._run_scan(base_path, run_pipeline)
            else:
                if not self.quiet:
                    print(f"  Target did not exist on base ref — treating as new")
                return self._wrap_all_as_new(target, head_findings, head_verdict)
        except Exception as exc:
            if self.verbose:
                print(f"  [WARNING] Base scan failed: {exc}")
                print("  Treating all HEAD findings as new")
            return self._wrap_all_as_new(target, head_findings, head_verdict)
        finally:
            if base_worktree:
                _cleanup_worktree(base_worktree, repo_root)

        # ── LLM impact analysis ──
        return self._analyze_impact(
            target, base_findings, head_findings,
            base_verdict, head_verdict,
            base_ref, head_ref, repo_root,
        )

    def _run_scan(self, target_path: str, run_pipeline) -> tuple[list[SecurityFinding], str]:
        """Run the scanner on a single path and return (findings, verdict)."""
        plugin, static_findings = self.scanner.scan_plugin(
            target_path, skip_static_analysis=not self.static,
        )
        findings, verdict = run_pipeline(
            scanner=self.scanner,
            plugin=plugin,
            static_findings=static_findings,
            ai_provider=self.ai_provider,
            ai_model=self.ai_model,
            verbose=self.verbose,
            quiet=self.quiet,
            static=self.static,
            ai_triage_threshold=self.ai_triage_threshold,
            workers=self.workers,
            rate_limiter=self.rate_limiter,
        )

        if not findings and plugin.components and not self.quiet:
            print(f"\n  [WARNING] Scan returned 0 findings for "
                  f"{len(plugin.components)} component(s).")
            print(f"  This may indicate the AI provider failed silently.")
            print(f"  Re-run with --verbose to see detailed errors.")

        return findings, verdict

    def _scan_new_target(
        self, target: AffectedTarget, head_path: str,
    ) -> TargetImpactResult:
        """All files are new — every finding is new."""
        from scanner.main import _run_single_scan_pipeline
        head_findings, head_verdict = self._run_scan(head_path, _run_single_scan_pipeline)
        return self._wrap_all_as_new(target, head_findings, head_verdict)

    def _scan_deleted_target(
        self, target: AffectedTarget, base_ref: str, repo_root: str,
    ) -> TargetImpactResult:
        """Entire target deleted — all previous findings resolved."""
        return TargetImpactResult(
            target=target,
            impact_summary="Target deleted — all previous findings resolved",
            risk_delta="decreased",
            head_verdict="safe",
            base_verdict=None,
        )

    def _wrap_all_as_new(
        self,
        target: AffectedTarget,
        findings: list[SecurityFinding],
        verdict: str,
    ) -> TargetImpactResult:
        new_findings = [
            ImpactFinding(
                status="new",
                finding=f,
                description="New code — no base version to compare",
                severity=f.severity,
            )
            for f in findings
        ]
        malicious = sum(1 for f in findings if f.section == "malicious")
        return TargetImpactResult(
            target=target,
            new_findings=new_findings,
            impact_summary=(
                f"New {target.target_type}: {len(findings)} finding(s), "
                f"{malicious} malicious"
            ),
            risk_delta="increased" if findings else "unchanged",
            head_verdict=verdict,
        )

    # ── LLM impact analysis ──────────────────────────────────────────

    def _analyze_impact(
        self,
        target: AffectedTarget,
        base_findings: list[SecurityFinding],
        head_findings: list[SecurityFinding],
        base_verdict: Optional[str],
        head_verdict: str,
        base_ref: str,
        head_ref: str,
        repo_root: str,
    ) -> TargetImpactResult:
        """Use the LLM to semantically correlate base/HEAD findings."""
        if not self.quiet:
            print(f"\n  Running LLM impact analysis...")
            print(f"    Base findings: {len(base_findings)}")
            print(f"    HEAD findings: {len(head_findings)}")

        code_diffs = _collect_patches(target, base_ref, head_ref, repo_root)
        changed_files_str = ", ".join(cf.path for cf in target.changed_files)

        user_prompt = _IMPACT_USER_PROMPT.format(
            target_name=target.name,
            target_type=target.target_type,
            change_scenario=target.change_scenario,
            changed_files=changed_files_str,
            code_diffs=code_diffs,
            base_findings_json=_findings_to_json(base_findings),
            head_findings_json=_findings_to_json(head_findings),
        )

        messages = [
            {"role": "system", "content": _IMPACT_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ]

        try:
            response = invoke_with_retry(
                self.llm, messages,
                max_retries=2,
                rate_limiter=self.rate_limiter,
                verbose=self.verbose,
            )
            raw = extract_text_content(response.content)

            text = raw.strip()
            if text.startswith("```"):
                first_nl = text.index("\n")
                last_fence = text.rfind("```")
                text = text[first_nl + 1:last_fence].strip()

            impact_data = json.loads(text)
            return self._parse_impact_response(
                impact_data, target, base_findings, head_findings,
                base_verdict, head_verdict,
            )

        except Exception as exc:
            if self.verbose:
                print(f"  [WARNING] LLM impact analysis failed: {exc}")
                print("  Falling back to heuristic matching...")
            return self._heuristic_impact(
                target, base_findings, head_findings,
                base_verdict, head_verdict,
            )

    def _parse_impact_response(
        self,
        data: dict,
        target: AffectedTarget,
        base_findings: list[SecurityFinding],
        head_findings: list[SecurityFinding],
        base_verdict: Optional[str],
        head_verdict: str,
    ) -> TargetImpactResult:
        """Parse the LLM impact analysis JSON into a TargetImpactResult."""
        result = TargetImpactResult(
            target=target,
            impact_summary=data.get("impact_summary", ""),
            risk_delta=data.get("risk_delta", "unchanged"),
            head_verdict=head_verdict,
            base_verdict=base_verdict,
        )

        # Parse new vulnerabilities
        for entry in data.get("new_vulnerabilities", []):
            idx = entry.get("head_finding_index")
            head_f = head_findings[idx] if idx is not None and idx < len(head_findings) else None
            sev = entry.get("severity", "medium")
            if sev not in ("critical", "high", "medium", "low"):
                sev = "medium"

            if head_f:
                finding = head_f
            else:
                finding = SecurityFinding(
                    severity=sev,
                    rule_id=f"ci-impact-new-{entry.get('category', 'unknown')}",
                    rule_name=entry.get("title", "New vulnerability"),
                    message=entry.get("description", ""),
                    component_type=target.target_type,
                    component_name=entry.get("component", target.name),
                    component_path=entry.get("file_path", ""),
                    recommendation=entry.get("remediation"),
                    section=entry.get("section", "code_security"),
                    category=entry.get("category"),
                    line=entry.get("line_number"),
                    snippet=entry.get("code_snippet"),
                )

            result.new_findings.append(ImpactFinding(
                status="new",
                finding=finding,
                description=entry.get("description", ""),
                severity=sev,
            ))

        # Parse worsened vulnerabilities
        for entry in data.get("worsened_vulnerabilities", []):
            h_idx = entry.get("head_finding_index")
            b_idx = entry.get("base_finding_index")
            head_f = head_findings[h_idx] if h_idx is not None and h_idx < len(head_findings) else None
            base_f = base_findings[b_idx] if b_idx is not None and b_idx < len(base_findings) else None
            sev = entry.get("severity", "medium")
            if sev not in ("critical", "high", "medium", "low"):
                sev = "medium"

            finding = head_f or (base_f if base_f else SecurityFinding(
                severity=sev,
                rule_id="ci-impact-worsened",
                rule_name=entry.get("title", "Worsened vulnerability"),
                message=entry.get("description", ""),
                component_type=target.target_type,
                component_name=target.name,
                component_path="",
            ))

            desc = entry.get("description", "")
            detail = entry.get("change_detail", "")
            if detail:
                desc = f"{desc} — {detail}" if desc else detail

            result.worsened_findings.append(ImpactFinding(
                status="worsened",
                finding=finding,
                description=desc,
                severity=sev,
                paired_finding=base_f,
            ))

        # Parse resolved vulnerabilities
        for entry in data.get("resolved_vulnerabilities", []):
            b_idx = entry.get("base_finding_index")
            base_f = base_findings[b_idx] if b_idx is not None and b_idx < len(base_findings) else None

            if base_f:
                result.resolved_findings.append(ImpactFinding(
                    status="resolved",
                    finding=base_f,
                    description=entry.get("description", ""),
                    severity=base_f.severity,
                ))

        # Parse unchanged vulnerabilities
        for entry in data.get("unchanged_vulnerabilities", []):
            h_idx = entry.get("head_finding_index")
            b_idx = entry.get("base_finding_index")
            head_f = head_findings[h_idx] if h_idx is not None and h_idx < len(head_findings) else None
            base_f = base_findings[b_idx] if b_idx is not None and b_idx < len(base_findings) else None
            finding = head_f or base_f

            if finding:
                result.unchanged_findings.append(ImpactFinding(
                    status="unchanged",
                    finding=finding,
                    description=entry.get("description", ""),
                    severity=finding.severity,
                    paired_finding=base_f if head_f else head_f,
                ))

        return result

    # ── Heuristic fallback ───────────────────────────────────────────

    def _heuristic_impact(
        self,
        target: AffectedTarget,
        base_findings: list[SecurityFinding],
        head_findings: list[SecurityFinding],
        base_verdict: Optional[str],
        head_verdict: str,
    ) -> TargetImpactResult:
        """Best-effort heuristic matching when the LLM is unavailable.

        Groups findings by (category, component_type, component_name) and
        uses simple text overlap to attempt correlation.
        """
        result = TargetImpactResult(
            target=target,
            head_verdict=head_verdict,
            base_verdict=base_verdict,
        )

        def _key(f: SecurityFinding) -> str:
            return f"{f.category or ''}|{f.component_type}|{f.component_name}"

        base_by_key: dict[str, list[SecurityFinding]] = {}
        for f in base_findings:
            base_by_key.setdefault(_key(f), []).append(f)

        matched_base: set[int] = set()

        for hf in head_findings:
            key = _key(hf)
            candidates = base_by_key.get(key, [])
            best_match = None
            best_score = 0.0

            for bf in candidates:
                if id(bf) in matched_base:
                    continue
                score = _text_similarity(hf.message, bf.message)
                if score > best_score:
                    best_score = score
                    best_match = bf

            if best_match and best_score > 0.4:
                matched_base.add(id(best_match))
                result.unchanged_findings.append(ImpactFinding(
                    status="unchanged",
                    finding=hf,
                    description="Heuristic match — likely same issue",
                    severity=hf.severity,
                    paired_finding=best_match,
                ))
            else:
                result.new_findings.append(ImpactFinding(
                    status="new",
                    finding=hf,
                    description="Potentially new (heuristic — no confident match in base)",
                    severity=hf.severity,
                ))

        for bf in base_findings:
            if id(bf) not in matched_base:
                result.resolved_findings.append(ImpactFinding(
                    status="resolved",
                    finding=bf,
                    description="Not found in HEAD scan (heuristic)",
                    severity=bf.severity,
                ))

        new_mal = sum(1 for f in result.new_findings if f.finding.section == "malicious")
        resolved_mal = sum(1 for f in result.resolved_findings if f.finding.section == "malicious")

        if new_mal > resolved_mal:
            result.risk_delta = "increased"
        elif resolved_mal > new_mal:
            result.risk_delta = "decreased"
        else:
            result.risk_delta = "unchanged"

        result.impact_summary = (
            f"Heuristic analysis: {len(result.new_findings)} potentially new, "
            f"{len(result.resolved_findings)} potentially resolved, "
            f"{len(result.unchanged_findings)} unchanged"
        )

        return result

    # ── Summary computation ──────────────────────────────────────────

    def _compute_summary(self, result: PRScanResult) -> PRScanSummary:
        total_new = sum(len(r.new_findings) for r in result.target_results)
        total_worsened = sum(len(r.worsened_findings) for r in result.target_results)
        total_resolved = sum(len(r.resolved_findings) for r in result.target_results)
        total_unchanged = sum(len(r.unchanged_findings) for r in result.target_results)

        # Determine overall risk delta
        increased = sum(1 for r in result.target_results if r.risk_delta == "increased")
        decreased = sum(1 for r in result.target_results if r.risk_delta == "decreased")
        if increased > decreased:
            overall_delta = "increased"
        elif decreased > increased:
            overall_delta = "decreased"
        else:
            overall_delta = "unchanged"

        # Fail if any new or worsened malicious findings
        has_actionable_malicious = False
        for r in result.target_results:
            for f in r.new_findings + r.worsened_findings:
                if f.finding.section == "malicious":
                    has_actionable_malicious = True
                    break

        return PRScanSummary(
            total_targets_affected=len(result.target_results),
            new_count=total_new,
            worsened_count=total_worsened,
            resolved_count=total_resolved,
            unchanged_count=total_unchanged,
            overall_risk_delta=overall_delta,
            verdict="fail" if has_actionable_malicious else "pass",
        )


# ── Text similarity helper ───────────────────────────────────────────


def _text_similarity(a: str, b: str) -> float:
    """Simple word-overlap Jaccard similarity."""
    if not a or not b:
        return 0.0
    words_a = set(a.lower().split())
    words_b = set(b.lower().split())
    if not words_a or not words_b:
        return 0.0
    intersection = words_a & words_b
    union = words_a | words_b
    return len(intersection) / len(union)
